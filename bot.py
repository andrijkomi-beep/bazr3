import requests
from bs4 import BeautifulSoup
import json
import time
import re

# =========================
# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
# =========================
TOKEN = "8469023268:AAEi-dahnEE0XzsuroEA2xLkf1KtbYg81Aw"
CHAT_ID = "453173481"
BASE_URL = "https://auto.bazos.sk/"
CATEGORY = "auta"  # –º–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏ –Ω–∞ –ø–æ—Ç—Ä—ñ–±–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é
MIN_PRICE = 500
CHECK_INTERVAL = 180  # —Å–µ–∫—É–Ω–¥ –º—ñ–∂ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞–º–∏
NUM_PAGES = 50       # —Å–∫—ñ–ª—å–∫–∏ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏
SAVE_FILE = "seen_ads.json"

# =========================
# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó
# =========================
try:
    with open(SAVE_FILE, "r") as f:
        seen_ids = set(json.load(f))
except:
    seen_ids = set()

# =========================
# –§—É–Ω–∫—Ü—ñ—ó
# =========================
def send_message(text):
    api_url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    requests.post(api_url, data={
        "chat_id": CHAT_ID,
        "text": text
    })

def save_seen():
    with open(SAVE_FILE, "w") as f:
        json.dump(list(seen_ids), f)

def extract_price(text):
    nums = re.findall(r"\d+", text.replace(" ", ""))
    if nums:
        return int("".join(nums))
    return 0

# =========================
# –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª
# =========================
print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω–∏–π...")

while True:
    try:
        for page in range(1, NUM_PAGES + 1):
            url = f"{BASE_URL}{CATEGORY}/?page={page}"
            resp = requests.get(url, timeout=10)
            soup = BeautifulSoup(resp.text, "html.parser")

            # –ö–æ–∂–Ω–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
            ads = soup.find_all("div", class_="inzerat")  # –æ—Å–Ω–æ–≤–Ω–∏–π –±–ª–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
            for ad in ads:
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç–æ–ø–æ–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
                if ad.find(class_="top"):
                    continue

                # –û—Ç—Ä–∏–º—É—î–º–æ ID –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                link_tag = ad.find("a", href=True)
                if not link_tag:
                    continue
                link = link_tag["href"]
                ad_id_match = re.search(r'/(\d+)\.html', link)
                if not ad_id_match:
                    continue
                ad_id = ad_id_match.group(1)

                if ad_id in seen_ids:
                    continue

                # –û—Ç—Ä–∏–º—É—î–º–æ title —Ç–∞ —Ü—ñ–Ω—É
                title_tag = ad.find("h3")
                title = title_tag.text.strip() if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∏"

                price_tag = ad.find("p", class_="cena")
                price = extract_price(price_tag.text) if price_tag else 0
                if price < MIN_PRICE:
                    continue

                # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
                msg = f"üöó –ù–æ–≤–µ –∞–≤—Ç–æ ({price}‚Ç¨+)\n\n{title}\nüí∞ {price}‚Ç¨\nüîó {link}"
                send_message(msg)
                print("–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ:", title)

                # –î–æ–¥–∞—î–º–æ –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó
                seen_ids.add(ad_id)
                save_seen()

    except Exception as e:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞:", e)

    time.sleep(CHECK_INTERVAL)